{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Selected_Ass_NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hYt2HneDndVY"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Activation , Dropout\n",
        "from keras import utils as np_utils\n",
        "import keras.preprocessing.text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "dataframe = pd.read_excel('AJGT.xlsx', index_col=0) \n",
        "dataframe.drop_duplicates(inplace = True)\n",
        "data= dataframe\n",
        "\n",
        "print(dataframe.head())\n",
        "\n",
        "print(type(data['Feed']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DBK5ZpkocQE",
        "outputId": "59bcee16-1535-4e50-ed0a-f20d27f04b9c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 Feed Sentiment\n",
            "ID                                                             \n",
            "1    اربد فيها جامعات اكثر من عمان ... وفيها قد عم...  Positive\n",
            "2    الحلو انكم بتحكوا على اساس انو الاردن ما فيه ...  Negative\n",
            "3                             كله رائع بجد ربنا يكرمك  Positive\n",
            "4                                  لسانك قذر يا قمامه  Negative\n",
            "5   ​انا داشره وغير متزوجه ولدي علاقات مشبوه واحشش...  Negative\n",
            "<class 'pandas.core.series.Series'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(len(data) * .7)\n",
        "test_size =  int(len(data) * .3)\n",
        "\n",
        "print(int(len(data['Feed'])))\n",
        "print(train_size)\n",
        "print(test_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgArZOjIsR9Q",
        "outputId": "6e8f383d-9f9d-4aa4-b467-9f2f93cedddb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1800\n",
            "1260\n",
            "540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts= data['Feed']\n",
        "tags = data['Sentiment']\n",
        "\n",
        "train_posts = data['Feed'][:train_size]\n",
        "train_tags = data['Sentiment'][:train_size]\n",
        "\n",
        "\n",
        "test_posts = data['Feed'][train_size:]\n",
        "test_tags =  data['Sentiment'][train_size:]"
      ],
      "metadata": {
        "id": "47iR5WaeSjzG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=None,lower=False)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# tfidf: the value of the word in the text\n",
        "# tokanizer: to give every word in the text and convert it to matrix to fit in the model\n",
        "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
        "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n"
      ],
      "metadata": {
        "id": "MRnr1Q5Stjgf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(tags)\n",
        "tagst=encoder.fit_transform(tags)\n",
        "\n",
        "num_classes = int((len(set(tagst))))\n",
        "print((len(set(tagst))))\n",
        "\n",
        "y_train = encoder.fit_transform(train_tags)\n",
        "y_test = encoder.fit_transform(test_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lyd7Qhztm0_",
        "outputId": "c9f85b5e-6415-4c21-b6c0-b549a20dadae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train= keras.utils.np_utils.to_categorical(y_train,num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "num_labels = int(len(y_train.shape))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_words=vocab_size\n",
        "print(max_words)"
      ],
      "metadata": {
        "id": "7Rkj-GJftriA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a6851bf-9575-44bd-dbe1-f2d03ca6e29c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=150,\n",
        "                    epochs=4,\n",
        "                    verbose=1)\n",
        "            \n",
        "\n",
        "\n",
        "model.save('NLP_Model.h1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wH2g--0v1B9",
        "outputId": "8318f6e9-3f5e-4364-f48d-a006b441451d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "9/9 [==============================] - 2s 12ms/step - loss: 0.6212 - accuracy: 0.6540\n",
            "Epoch 2/4\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1825 - accuracy: 0.9857\n",
            "Epoch 3/4\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.0659 - accuracy: 0.9968\n",
            "Epoch 4/4\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.0279 - accuracy: 0.9992\n",
            "INFO:tensorflow:Assets written to: NLP_Model.h1/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# saving\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# loading\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "metadata": {
        "id": "mcL5p_tDxMhv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = keras.models.load_model('NLP_Model.h1')\n",
        "Evaluation_valus = model.evaluate(x_test,y_test,verbose=0)\n",
        "print('accuracy = ',Evaluation_valus[1]*100,'%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXAtr97NxOJr",
        "outputId": "0ca25504-3435-44cf-9558-a0788cf3f02e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  84.44444537162781 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in data[\"Feed\"][:25]:\n",
        "\n",
        "    tokens = tokenizer.texts_to_matrix([x], mode='tfidf')\n",
        "\n",
        "    c=model.predict(np.array(tokens))\n",
        "    predict_x=model.predict(tokens) \n",
        "    classes_x=np.argmax(predict_x,axis=1)\n",
        "    xc = encoder.inverse_transform(classes_x)\n",
        "\n",
        "\n",
        "    print(c,\"= \\t\",classes_x,\"\\t\",xc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsgHtUndx-sB",
        "outputId": "614b1968-70e2-489d-8c13-b8d9af2ae1f9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.03079776 0.9941631 ]] = \t [1] \t ['Positive']\n",
            "[[0.9700677  0.02797674]] = \t [0] \t ['Negative']\n",
            "[[0.08399869 0.96744496]] = \t [1] \t ['Positive']\n",
            "[[0.84861904 0.20516473]] = \t [0] \t ['Negative']\n",
            "[[0.99832505 0.00162815]] = \t [0] \t ['Negative']\n",
            "[[0.01823431 0.99142784]] = \t [1] \t ['Positive']\n",
            "[[0.9869545  0.01593387]] = \t [0] \t ['Negative']\n",
            "[[0.9723547  0.02275095]] = \t [0] \t ['Negative']\n",
            "[[3.3549193e-04 9.9992692e-01]] = \t [1] \t ['Positive']\n",
            "[[0.12324993 0.9211591 ]] = \t [1] \t ['Positive']\n",
            "[[0.9629061  0.04437867]] = \t [0] \t ['Negative']\n",
            "[[0.9647412  0.06022137]] = \t [0] \t ['Negative']\n",
            "[[0.9762832  0.05952402]] = \t [0] \t ['Negative']\n",
            "[[0.90849984 0.17191936]] = \t [0] \t ['Negative']\n",
            "[[0.97406954 0.01847858]] = \t [0] \t ['Negative']\n",
            "[[0.15092535 0.8920725 ]] = \t [1] \t ['Positive']\n",
            "[[0.89923126 0.09626542]] = \t [0] \t ['Negative']\n",
            "[[0.9708374  0.02892057]] = \t [0] \t ['Negative']\n",
            "[[0.862549   0.13951658]] = \t [0] \t ['Negative']\n",
            "[[0.18026152 0.89415383]] = \t [1] \t ['Positive']\n",
            "[[0.11033291 0.95007753]] = \t [1] \t ['Positive']\n",
            "[[0.00863808 0.993725  ]] = \t [1] \t ['Positive']\n",
            "[[0.12272171 0.9222232 ]] = \t [1] \t ['Positive']\n",
            "[[0.02322387 0.9850987 ]] = \t [1] \t ['Positive']\n",
            "[[0.8977641  0.13886546]] = \t [0] \t ['Negative']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test_input='هذا المحتوى سخيف جدا'\n",
        "#test_input='هذا المحتوى راقى جدا'\n",
        "test_input='هذا المحتوى جميل جدا'\n",
        "input= tokenizer.texts_to_matrix([test_input],mode='tfidf')\n",
        "c=model.predict(np.array(input))\n",
        "predict_x=model.predict(input) \n",
        "classes_x=np.argmax(predict_x,axis=1)\n",
        "xc = encoder.inverse_transform(classes_x)\n",
        "\n",
        "print(c,\"= \\t\",classes_x,\"\\t\",xc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1kdorl2yjFq",
        "outputId": "d5c86e27-c922-4c52-9788-b09e13c96578"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.27673876 0.8016861 ]] = \t [1] \t ['Positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#texts= data['Feed']\n",
        "#tags = data['Sentiment']\n",
        "\n",
        "#texts=np.array(texts)\n",
        "#tags=np.array(tags)\n",
        "\n",
        "#Texts=[]\n",
        "#for i in range(1800):\n",
        "#   Sentiment= tags[i]\n",
        "#   Texts.append((texts[i],Sentiment))\n",
        "\n",
        "#import random\n",
        "#random.shuffle(Texts)\n",
        "#print(Texts)\n",
        "\n",
        "#all_words = nltk.FreqDist(w for w in texts)\n",
        "#word_features = list(all_words)[:1800] \n",
        "#for i in range(500):\n",
        "#  print(word_features[0])\n",
        "\n",
        "\n",
        "#def document_features(document): \n",
        "#    document_words = set(document) \n",
        "#    features = {}\n",
        "#    for word in word_features:\n",
        "#        features['contains({})'.format(word)] = (word in document_words)\n",
        "#    return features\n",
        "\n",
        "#document_features(Texts)\n",
        "\n",
        "#featuresets = [(document_features(d), c) for (d,c) in Texts]\n",
        "#train_set, test_set = featuresets[:train_size], featuresets[train_size:]\n",
        "\n",
        "#classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "#print(nltk.classify.accuracy(classifier, test_set)) "
      ],
      "metadata": {
        "id": "wRJkt9uFtG4E"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}